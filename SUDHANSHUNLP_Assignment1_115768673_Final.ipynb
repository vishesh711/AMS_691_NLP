{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishesh711/AMS_691_NLP-HW1/blob/main/SUDHANSHUNLP_Assignment1_115768673_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABmuEiKlNg8o",
        "outputId": "0cce5d8e-1bcf-41de-f11e-95a08f6d1117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Navigate to your file\n",
        "file_path = '/content/drive/MyDrive/hw1-data'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuHFdTMqQjg0",
        "outputId": "fab4f853-dc0f-4f4c-f158-6430e0bb258b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy scipy\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy.stats import spearmanr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4mNi5uIE3ze"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def load_vocab(file_path):\n",
        "    \"\"\"Loads a vocabulary file where each line contains a word.\"\"\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        vocab = set([line.strip() for line in file])\n",
        "    return vocab\n",
        "\n",
        "def count_word_pairs(corpus_file, vocab_V, vocab_VC, window_size):\n",
        "    \"\"\"\n",
        "    Count the number of times word y appears in a context window of size w centered at word x.\n",
        "    :param corpus_file: Path to the corpus file.\n",
        "    :param vocab_V: Vocabulary set for center words.\n",
        "    :param vocab_VC: Vocabulary set for context words.\n",
        "    :param window_size: Size of the context window (w).\n",
        "    :return: A dictionary of word pair counts.\n",
        "    \"\"\"\n",
        "    # Sparse data structure to store counts\n",
        "    word_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    # Process the corpus\n",
        "    with open(corpus_file, 'r') as file:\n",
        "        for line in file:\n",
        "            words = line.strip().split()  # Tokenize the sentence\n",
        "            for i, word in enumerate(words):\n",
        "                if word not in vocab_V:\n",
        "                    continue  # Skip words not in vocab_V (not valid center words)\n",
        "\n",
        "                # Define context window\n",
        "                start = max(i - window_size, 0)  # Ensure window doesn't go out of bounds\n",
        "                end = min(i + window_size + 1, len(words))  # Same for the other side\n",
        "\n",
        "                # Count co-occurrences\n",
        "                for j in range(start, end):\n",
        "                    if i == j:  # Skip the center word itself\n",
        "                        continue\n",
        "                    context_word = words[j]\n",
        "                    if context_word in vocab_VC:\n",
        "                        word_counts[word][context_word] += 1\n",
        "\n",
        "    return word_counts\n",
        "\n",
        "# Load vocabularies for V and VC\n",
        "vocab_V = load_vocab('/content/drive/MyDrive/hw1-data/vocab-15kws.txt')  # Center words\n",
        "vocab_VC = load_vocab('/content/drive/MyDrive/hw1-data/vocab-5k.txt')    # Context words\n",
        "\n",
        "# Perform word pair counting for w = 3 and w = 6\n",
        "word_counts_w1 = count_word_pairs('/content/drive/MyDrive/hw1-data/wiki-1percent.txt', vocab_V, vocab_VC, 1)\n",
        "word_counts_w3 = count_word_pairs('/content/drive/MyDrive/hw1-data/wiki-1percent.txt', vocab_V, vocab_VC, 3)\n",
        "word_counts_w6 = count_word_pairs('/content/drive/MyDrive/hw1-data/wiki-1percent.txt', vocab_V, vocab_VC, 6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OCvUiw0GHvc",
        "outputId": "db31157a-12d6-4f83-9455-ff35fa2ff047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts for w = 3:\n",
            "Count for (chicken, the): 52\n",
            "Count for (chicken, wings): 6\n",
            "Count for (chicago, chicago): 38\n",
            "Count for (coffee, the): 95\n",
            "Count for (coffee, cup): 10\n",
            "Count for (coffee, coffee): 4\n",
            "\n",
            "Counts for w = 6:\n",
            "Count for (chicken, the): 103\n",
            "Count for (chicken, wings): 7\n",
            "Count for (chicago, chicago): 122\n",
            "Count for (coffee, the): 201\n",
            "Count for (coffee, cup): 14\n",
            "Count for (coffee, coffee): 36\n"
          ]
        }
      ],
      "source": [
        "def report_word_pair_counts(word_counts, word_pairs):\n",
        "    \"\"\"\n",
        "    Report the counts for a set of specific word pairs.\n",
        "    :param word_counts: The nested dictionary of word counts.\n",
        "    :param word_pairs: List of word pairs (x, y) to report counts for.\n",
        "    :return: Prints the counts for the word pairs.\n",
        "    \"\"\"\n",
        "    for word1, word2 in word_pairs:\n",
        "        count = word_counts.get(word1, {}).get(word2, 0)  # Default to 0 if pair doesn't exist\n",
        "        print(f\"Count for ({word1}, {word2}): {count}\")\n",
        "\n",
        "# List of word pairs to check\n",
        "word_pairs = [\n",
        "    ('chicken', 'the'),\n",
        "    ('chicken', 'wings'),\n",
        "    ('chicago', 'chicago'),\n",
        "    ('coffee', 'the'),\n",
        "    ('coffee', 'cup'),\n",
        "    ('coffee', 'coffee')\n",
        "]\n",
        "\n",
        "# Report counts for w = 3\n",
        "print(\"Counts for w = 3:\")\n",
        "report_word_pair_counts(word_counts_w3, word_pairs)\n",
        "\n",
        "# Report counts for w = 6\n",
        "print(\"\\nCounts for w = 6:\")\n",
        "report_word_pair_counts(word_counts_w6, word_pairs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvkQiMrXBUbM",
        "outputId": "b838a3a7-e4b4-4333-e384-7eab15d30ae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman correlation for MEN dataset (w = 3): 0.2251396048448754\n",
            "Spearman correlation for SimLex-999 dataset (w = 3): 0.05876135331349779\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Function to compute cosine similarity between two word vectors\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = sum(vec1[key] * vec2.get(key, 0) for key in vec1)\n",
        "    norm1 = np.sqrt(sum(val ** 2 for val in vec1.values()))\n",
        "    norm2 = np.sqrt(sum(val ** 2 for val in vec2.values()))\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0  # Return zero if either vector is zero-length\n",
        "    return dot_product / (norm1 * norm2)\n",
        "\n",
        "# Function to load word similarity datasets (MEN or SimLex-999)\n",
        "def load_similarity_dataset(file_path):\n",
        "    word_pairs = []\n",
        "    human_scores = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        next(file)  # Skip the header if the file has one\n",
        "        for line in file:\n",
        "            word1, word2, score = line.strip().split()\n",
        "            word_pairs.append((word1, word2))\n",
        "            human_scores.append(float(score))\n",
        "    return word_pairs, human_scores\n",
        "\n",
        "# Function to get word vector from counts\n",
        "def get_word_vector(word, word_counts):\n",
        "    # Return the word vector if it exists, otherwise return an empty dict\n",
        "    return word_counts.get(word, {})\n",
        "\n",
        "# Function to evaluate word vectors on a dataset\n",
        "def evaluate_word_vectors(dataset_path, word_counts):\n",
        "    word_pairs, human_scores = load_similarity_dataset(dataset_path)\n",
        "    model_scores = []\n",
        "\n",
        "    for word1, word2 in word_pairs:\n",
        "        vec1 = get_word_vector(word1, word_counts)\n",
        "        vec2 = get_word_vector(word2, word_counts)\n",
        "        similarity = cosine_similarity(vec1, vec2)  # Compute cosine similarity\n",
        "        model_scores.append(similarity)\n",
        "\n",
        "    # Compute Spearman's rank correlation between human scores and model scores\n",
        "    spearman_corr, _ = spearmanr(human_scores, model_scores)\n",
        "    return spearman_corr\n",
        "\n",
        "# Load MEN and SimLex-999 datasets\n",
        "men_dataset = '/content/drive/MyDrive/hw1-data/men.txt'\n",
        "simlex_dataset = '/content/drive/MyDrive/hw1-data/simlex-999.txt'\n",
        "\n",
        "# Evaluate word vectors (for w = 3, using vocab-15kws.txt for V and vocab-5k.txt for VC)\n",
        "spearman_corr_men = evaluate_word_vectors(men_dataset, word_counts_w3)\n",
        "spearman_corr_simlex = evaluate_word_vectors(simlex_dataset, word_counts_w3)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Spearman correlation for MEN dataset (w = 3): {spearman_corr_men}\")\n",
        "print(f\"Spearman correlation for SimLex-999 dataset (w = 3): {spearman_corr_simlex}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJNQ4aFiNEmT",
        "outputId": "b61eb462-bfb4-4952-a2b5-4194d77b24ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman correlation for MEN dataset (w = 3, IDF): 0.47285107067097226\n",
            "Spearman correlation for SimLex-999 dataset (w = 3, IDF): 0.16436460954112178\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Function to count occurrences of each word in sentences\n",
        "def count_word_sentence_occurrences(corpus_file):\n",
        "    sentence_counts = defaultdict(int)\n",
        "    with open(corpus_file, 'r') as file:\n",
        "        for line in file:\n",
        "            words = set(line.strip().split())  # Treat sentence as a set of unique words\n",
        "            for word in words:\n",
        "                sentence_counts[word] += 1\n",
        "    return sentence_counts\n",
        "\n",
        "# Function to calculate IDF-weighted word vectors\n",
        "def calculate_idf_weighted_vectors(corpus_file, word_counts, vocab_VC):\n",
        "    # Count the total number of sentences\n",
        "    total_sentences = sum(1 for _ in open(corpus_file, 'r'))\n",
        "\n",
        "    # Count the number of sentences each word in the context vocabulary appears in\n",
        "    sentence_counts = count_word_sentence_occurrences(corpus_file)\n",
        "\n",
        "    # Calculate IDF-weighted vectors\n",
        "    idf_weighted_vectors = defaultdict(lambda: defaultdict(float))\n",
        "    for word_x, context_dict in word_counts.items():\n",
        "        for word_y, count in context_dict.items():\n",
        "            # Compute IDF\n",
        "            idf = total_sentences / (sentence_counts[word_y] + 1)  # Avoid division by zero\n",
        "            # Apply IDF to the count\n",
        "            idf_weighted_vectors[word_x][word_y] = count * idf\n",
        "\n",
        "    return idf_weighted_vectors\n",
        "\n",
        "\n",
        "# Calculate the IDF-weighted vectors for w = 3\n",
        "idf_word_counts_w1 = calculate_idf_weighted_vectors('/content/drive/MyDrive/hw1-data/wiki-1percent.txt', word_counts_w1, vocab_VC)\n",
        "idf_word_counts_w3 = calculate_idf_weighted_vectors('/content/drive/MyDrive/hw1-data/wiki-1percent.txt', word_counts_w3, vocab_VC)\n",
        "idf_word_counts_w6 = calculate_idf_weighted_vectors('/content/drive/MyDrive/hw1-data/wiki-1percent.txt', word_counts_w6, vocab_VC)\n",
        "\n",
        "# Evaluate the IDF-weighted word vectors using EVALWS\n",
        "spearman_corr_idf_men_w3 = evaluate_word_vectors(men_dataset, idf_word_counts_w3)\n",
        "spearman_corr_idf_simlex_w3 = evaluate_word_vectors(simlex_dataset, idf_word_counts_w3)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Spearman correlation for MEN dataset (w = 3, IDF): {spearman_corr_idf_men_w3}\")\n",
        "print(f\"Spearman correlation for SimLex-999 dataset (w = 3, IDF): {spearman_corr_idf_simlex_w3}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wWjaMj3QlCW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNFh7OyjQi7W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3d3c1d5-9339-414b-a5c7-29f1919594ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 context words with largest PMIs for 'coffee':\n",
            "tea: 8.314281269835263\n",
            "drinking: 7.71405449753307\n",
            "costa: 7.672608125452632\n",
            "shop: 7.5688741687808\n",
            "shops: 7.363997029461911\n",
            "sugar: 6.704091030083928\n",
            "coffee: 6.6773826307729784\n",
            "mix: 6.252416014250404\n",
            "seattle: 6.04743034466904\n",
            "houses: 5.982077807291471\n",
            "\n",
            "Top 10 context words with smallest PMIs for 'coffee':\n",
            "he: -2.161333783867941\n",
            "be: -2.060526683360527\n",
            "this: -1.9005260281642615\n",
            "had: -1.8633009473814206\n",
            "not: -1.8321233786639186\n",
            "its: -1.7043825744297731\n",
            "after: -1.4689138836692859\n",
            "more: -1.3633112453095726\n",
            "when: -1.2681226416703757\n",
            "page: -1.2479218069054545\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from math import log2\n",
        "\n",
        "# Function to calculate PMI values for word pairs\n",
        "def calculate_pmi(word_counts, vocab_V, vocab_VC):\n",
        "    total_count = sum(sum(context_dict.values()) for context_dict in word_counts.values())  # Total N\n",
        "    pmi_vectors = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    # Compute marginal probabilities\n",
        "    p_x = defaultdict(float)\n",
        "    p_y = defaultdict(float)\n",
        "\n",
        "    for word_x, context_dict in word_counts.items():\n",
        "        p_x[word_x] = sum(context_dict.values()) / total_count\n",
        "        for word_y, count in context_dict.items():\n",
        "            p_y[word_y] += count / total_count\n",
        "\n",
        "    # Compute PMI for each word pair\n",
        "    for word_x, context_dict in word_counts.items():\n",
        "        for word_y, count in context_dict.items():\n",
        "            joint_prob = count / total_count\n",
        "            if joint_prob > 0 and p_x[word_x] > 0 and p_y[word_y] > 0:\n",
        "                pmi = log2(joint_prob / (p_x[word_x] * p_y[word_y]))\n",
        "                pmi_vectors[word_x][word_y] = pmi\n",
        "\n",
        "    return pmi_vectors\n",
        "\n",
        "# Compute PMI for w = 3\n",
        "pmi_word_counts_w1 = compute_pmi(word_counts_w1, vocab_V, vocab_VC)\n",
        "pmi_word_counts_w3 = compute_pmi(word_counts_w3, vocab_V, vocab_VC)\n",
        "pmi_word_counts_w6 = compute_pmi(word_counts_w6, vocab_V, vocab_VC)\n",
        "\n",
        "\n",
        "# Get PMI for \"coffee\"\n",
        "coffee_pmi = pmi_word_counts_w3.get(\"coffee\", {})\n",
        "largest_pmi = sorted(coffee_pmi.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "smallest_pmi = sorted(coffee_pmi.items(), key=lambda x: x[1])[:10]\n",
        "\n",
        "print(\"Top 10 context words with largest PMIs for 'coffee':\")\n",
        "for word, pmi in largest_pmi:\n",
        "    print(f\"{word}: {pmi}\")\n",
        "\n",
        "print(\"\\nTop 10 context words with smallest PMIs for 'coffee':\")\n",
        "for word, pmi in smallest_pmi:\n",
        "    print(f\"{word}: {pmi}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUy-69bdVKhd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a153d362-0f59-4fd1-f147-a5114a68bc00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman correlation for MEN dataset (w = 3, PMI): 0.46673753829720827\n",
            "Spearman correlation for SimLex-999 dataset (w = 3, PMI): 0.1860041939515375\n"
          ]
        }
      ],
      "source": [
        "def create_pmi_word_vectors(pmi_values, vocab_V, vocab_VC):\n",
        "    pmi_vectors = defaultdict(dict)\n",
        "    for x in vocab_V:\n",
        "        for y in vocab_VC:\n",
        "            pmi_vectors[x][y] = pmi_values[x].get(y, 0)\n",
        "    return pmi_vectors\n",
        "\n",
        "# Create PMI-based word vectors\n",
        "pmi_word_vectors_w1 = create_pmi_word_vectors(pmi_word_counts_w1, vocab_V, vocab_VC)\n",
        "pmi_word_vectors_w3 = create_pmi_word_vectors(pmi_word_counts_w3, vocab_V, vocab_VC)\n",
        "pmi_word_vectors_w6 = create_pmi_word_vectors(pmi_word_counts_w6, vocab_V, vocab_VC)\n",
        "\n",
        "# Evaluate PMI-based word vectors using EVALWS\n",
        "spearman_corr_pmi_men_w3 = evaluate_word_vectors(men_dataset, pmi_word_vectors_w3)\n",
        "spearman_corr_pmi_simlex_w3 = evaluate_word_vectors(simlex_dataset, pmi_word_vectors_w3)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Spearman correlation for MEN dataset (w = 3, PMI): {spearman_corr_pmi_men_w3}\")\n",
        "print(f\"Spearman correlation for SimLex-999 dataset (w = 3, PMI): {spearman_corr_pmi_simlex_w3}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLy3twXHVt0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecdd1d0c-183e-4392-800d-89984e0b4d3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Method  Window Size  MEN Correlation  SimLex Correlation\n",
            "0  Counts            1         0.209092            0.067786\n",
            "1  Counts            3         0.225140            0.058761\n",
            "2  Counts            6         0.241067            0.044696\n",
            "3     IDF            1         0.347589            0.189255\n",
            "4     IDF            3         0.472851            0.164365\n",
            "5     IDF            6         0.532401            0.110635\n",
            "6     PMI            1         0.439174            0.229926\n",
            "7     PMI            3         0.466738            0.186004\n",
            "8     PMI            6         0.474624            0.149922\n",
            "\n",
            "Best methods for MEN dataset by window size:\n",
            "  Method  Window Size  MEN Correlation  SimLex Correlation\n",
            "6    PMI            1         0.439174            0.229926\n",
            "4    IDF            3         0.472851            0.164365\n",
            "5    IDF            6         0.532401            0.110635\n",
            "\n",
            "Best methods for SimLex-999 dataset by window size:\n",
            "  Method  Window Size  MEN Correlation  SimLex Correlation\n",
            "6    PMI            1         0.439174            0.229926\n",
            "7    PMI            3         0.466738            0.186004\n",
            "8    PMI            6         0.474624            0.149922\n",
            "\n",
            "Average performance of each method across all window sizes:\n",
            "        Window Size  MEN Correlation  SimLex Correlation\n",
            "Method                                                  \n",
            "Counts     3.333333         0.225099            0.057081\n",
            "IDF        3.333333         0.450947            0.154751\n",
            "PMI        3.333333         0.460179            0.188617\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to evaluate word vectors across different methods and window sizes\n",
        "def evaluate_all_methods(men_dataset, simlex_dataset, word_counts_w1, word_counts_w3, word_counts_w6,\n",
        "                         idf_word_counts_w1, idf_word_counts_w3, idf_word_counts_w6,\n",
        "                         pmi_word_vectors_w1, pmi_word_vectors_w3, pmi_word_vectors_w6):\n",
        "\n",
        "    # List to store results\n",
        "    results = []\n",
        "\n",
        "    # Loop through each method and window size\n",
        "    for method, word_vectors_w1, word_vectors_w3, word_vectors_w6 in [\n",
        "        ('Counts', word_counts_w1, word_counts_w3, word_counts_w6),\n",
        "        ('IDF', idf_word_counts_w1, idf_word_counts_w3, idf_word_counts_w6),\n",
        "        ('PMI', pmi_word_vectors_w1, pmi_word_vectors_w3, pmi_word_vectors_w6)]:\n",
        "\n",
        "        for w, word_vectors in [(1, word_vectors_w1), (3, word_vectors_w3), (6, word_vectors_w6)]:\n",
        "            # Evaluate on MEN dataset\n",
        "            spearman_corr_men = evaluate_word_vectors(men_dataset, word_vectors)\n",
        "            # Evaluate on SimLex-999 dataset\n",
        "            spearman_corr_simlex = evaluate_word_vectors(simlex_dataset, word_vectors)\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                'Method': method,\n",
        "                'Window Size': w,\n",
        "                'MEN Correlation': spearman_corr_men,\n",
        "                'SimLex Correlation': spearman_corr_simlex\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Evaluate all combinations of methods and window sizes\n",
        "results = evaluate_all_methods(\n",
        "    men_dataset, simlex_dataset,\n",
        "    word_counts_w1, word_counts_w3, word_counts_w6,  # Raw counts for w = 1, 3, 6\n",
        "    idf_word_counts_w1, idf_word_counts_w3, idf_word_counts_w6,  # IDF for w = 1, 3, 6\n",
        "    pmi_word_vectors_w1, pmi_word_vectors_w3, pmi_word_vectors_w6  # PMI for w = 1, 3, 6\n",
        ")\n",
        "\n",
        "# Display results in a structured format\n",
        "df_results = pd.DataFrame(results)\n",
        "print(df_results)\n",
        "\n",
        "# Analyze the results for trends\n",
        "def analyze_trends(df_results):\n",
        "    # Find the best performing method for each window size\n",
        "    best_method_men = df_results.groupby('Window Size')['MEN Correlation'].idxmax()\n",
        "    best_method_simlex = df_results.groupby('Window Size')['SimLex Correlation'].idxmax()\n",
        "\n",
        "    # Display best methods for each window size for MEN and SimLex\n",
        "    print(\"\\nBest methods for MEN dataset by window size:\")\n",
        "    print(df_results.loc[best_method_men])\n",
        "\n",
        "    print(\"\\nBest methods for SimLex-999 dataset by window size:\")\n",
        "    print(df_results.loc[best_method_simlex])\n",
        "\n",
        "    # Average performance of each method\n",
        "    avg_performance = df_results.groupby('Method').mean()\n",
        "    print(\"\\nAverage performance of each method across all window sizes:\")\n",
        "    print(avg_performance)\n",
        "\n",
        "# Analyze the trends\n",
        "analyze_trends(df_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to convert PMI word vectors (dictionaries) to numerical vectors (arrays)\n",
        "def convert_to_vector(word, word_vectors, vocab_VC):\n",
        "    vector = np.zeros(len(vocab_VC))  # Initialize vector with zeros\n",
        "    for i, context_word in enumerate(vocab_VC):\n",
        "        vector[i] = word_vectors[word].get(context_word, 0)  # Fill in the PMI values\n",
        "    return vector\n",
        "\n",
        "# Updated cosine similarity function using vector representations\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "# Function to get the k nearest neighbors of a word using vector representation\n",
        "def get_nearest_neighbors(word, word_vectors, vocab_V, vocab_VC, k=10):\n",
        "    if word not in word_vectors:\n",
        "        print(f\"Word '{word}' not found in word vectors.\")\n",
        "        return []\n",
        "\n",
        "    word_vector = convert_to_vector(word, word_vectors, vocab_VC)\n",
        "\n",
        "    similarities = []\n",
        "    for other_word in vocab_V:\n",
        "        if other_word != word and other_word in word_vectors:\n",
        "            other_word_vector = convert_to_vector(other_word, word_vectors, vocab_VC)\n",
        "            similarity = cosine_similarity(word_vector, other_word_vector)\n",
        "            similarities.append((other_word, similarity))\n",
        "\n",
        "    # Sort by similarity and return the top k\n",
        "    nearest_neighbors = sorted(similarities, key=lambda x: x[1], reverse=True)[:k]\n",
        "    return nearest_neighbors\n",
        "\n",
        "# Function to print the nearest neighbors for a word with two different window sizes\n",
        "def compare_nearest_neighbors(word, word_vectors_w1, word_vectors_w6, vocab_V, vocab_VC, k=10):\n",
        "    print(f\"Nearest neighbors for '{word}' (w = 1):\")\n",
        "    neighbors_w1 = get_nearest_neighbors(word, word_vectors_w1, vocab_V, vocab_VC, k)\n",
        "    for neighbor, similarity in neighbors_w1:\n",
        "        print(f\"{neighbor}: {similarity:.4f}\")\n",
        "\n",
        "    print(f\"\\nNearest neighbors for '{word}' (w = 6):\")\n",
        "    neighbors_w6 = get_nearest_neighbors(word, word_vectors_w6, vocab_V, vocab_VC, k)\n",
        "    for neighbor, similarity in neighbors_w6:\n",
        "        print(f\"{neighbor}: {similarity:.4f}\")\n",
        "\n",
        "# Example: Compare nearest neighbors for the word \"judges\"\n",
        "word = \"judges\"\n",
        "compare_nearest_neighbors(word, pmi_word_vectors_w1, pmi_word_vectors_w6, vocab_V, vocab_VC)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWdm15MABtbF",
        "outputId": "58788552-8839-48ad-9575-54ca6580397a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest neighbors for 'judges' (w = 1):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-7e71a4f1c0d2>:12: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "judge: 0.2221\n",
            "players: 0.2141\n",
            "appeals: 0.1902\n",
            "officials: 0.1832\n",
            "ministers: 0.1821\n",
            "justices: 0.1794\n",
            "members: 0.1793\n",
            "leaders: 0.1737\n",
            "contestants: 0.1685\n",
            "unanimously: 0.1683\n",
            "\n",
            "Nearest neighbors for 'judges' (w = 6):\n",
            "judge: 0.3091\n",
            "jury: 0.2898\n",
            "appeals: 0.2761\n",
            "courts: 0.2755\n",
            "panel: 0.2742\n",
            "supreme: 0.2711\n",
            "contestants: 0.2583\n",
            "candidates: 0.2531\n",
            "appeal: 0.2506\n",
            "officials: 0.2499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of query words from different parts of speech\n",
        "query_words = {\n",
        "    \"nouns\": [\"judge\", \"court\"],\n",
        "    \"verbs\": [\"decide\", \"argue\"],\n",
        "    \"adjectives\": [\"big\", \"important\"],\n",
        "    \"prepositions\": [\"in\", \"on\"]\n",
        "}\n",
        "\n",
        "# Analyze nearest neighbors for each POS category\n",
        "def analyze_nearest_neighbors_by_pos(query_words, word_vectors_w1, word_vectors_w6, vocab_V, vocab_VC, k=10):\n",
        "    for pos, words in query_words.items():\n",
        "        print(f\"\\nAnalyzing {pos} words:\")\n",
        "        for word in words:\n",
        "            print(f\"\\nQuery word: '{word}'\")\n",
        "            compare_nearest_neighbors(word, word_vectors_w1, word_vectors_w6, vocab_V, vocab_VC, k)\n",
        "\n",
        "# Example: Analyze nearest neighbors for nouns, verbs, adjectives, and prepositions\n",
        "analyze_nearest_neighbors_by_pos(query_words, pmi_word_vectors_w1, pmi_word_vectors_w6, vocab_V, vocab_VC)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q84KuzfoD6YB",
        "outputId": "196b00bd-e796-4d78-d2db-60c08b7ae354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing nouns words:\n",
            "\n",
            "Query word: 'judge'\n",
            "Nearest neighbors for 'judge' (w = 1):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-7e71a4f1c0d2>:12: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "captain: 0.2701\n",
            "smith: 0.2405\n",
            "king: 0.2327\n",
            "professor: 0.2320\n",
            "george: 0.2301\n",
            "clarke: 0.2235\n",
            "judges: 0.2221\n",
            "joseph: 0.2196\n",
            "architect: 0.2136\n",
            "actor: 0.2116\n",
            "\n",
            "Nearest neighbors for 'judge' (w = 6):\n",
            "supreme: 0.3979\n",
            "attorney: 0.3812\n",
            "court: 0.3665\n",
            "governor: 0.3584\n",
            "mayor: 0.3280\n",
            "appeals: 0.3266\n",
            "lawyer: 0.3225\n",
            "secretary: 0.3216\n",
            "criminal: 0.3129\n",
            "chief: 0.3125\n",
            "\n",
            "Query word: 'court'\n",
            "Nearest neighbors for 'court' (w = 1):\n",
            "courts: 0.3719\n",
            "government: 0.2631\n",
            "council: 0.2617\n",
            "law: 0.2419\n",
            "judge: 0.2256\n",
            "commission: 0.2074\n",
            "tribunal: 0.2003\n",
            "committee: 0.1988\n",
            "state: 0.1980\n",
            "appeals: 0.1966\n",
            "\n",
            "Nearest neighbors for 'court' (w = 6):\n",
            "supreme: 0.4536\n",
            "judge: 0.3665\n",
            "law: 0.3444\n",
            "appeals: 0.3325\n",
            "president: 0.3160\n",
            "governor: 0.3045\n",
            "council: 0.3042\n",
            "courts: 0.2932\n",
            "office: 0.2895\n",
            "government: 0.2863\n",
            "\n",
            "Analyzing verbs words:\n",
            "\n",
            "Query word: 'decide'\n",
            "Nearest neighbors for 'decide' (w = 1):\n",
            "choose: 0.3286\n",
            "understand: 0.3199\n",
            "determine: 0.3091\n",
            "observe: 0.2880\n",
            "tell: 0.2868\n",
            "remember: 0.2850\n",
            "specify: 0.2849\n",
            "explain: 0.2839\n",
            "ask: 0.2827\n",
            "know: 0.2825\n",
            "\n",
            "Nearest neighbors for 'decide' (w = 6):\n",
            "decides: 0.3663\n",
            "ask: 0.3427\n",
            "let: 0.3252\n",
            "try: 0.3230\n",
            "wish: 0.3179\n",
            "wants: 0.3076\n",
            "choose: 0.3044\n",
            "'ll: 0.3039\n",
            "whatever: 0.3013\n",
            "discuss: 0.2978\n",
            "\n",
            "Query word: 'argue'\n",
            "Nearest neighbors for 'argue' (w = 1):\n",
            "worry: 0.3076\n",
            "say: 0.2928\n",
            "disagree: 0.2774\n",
            "believe: 0.2625\n",
            "agree: 0.2613\n",
            "conclude: 0.2556\n",
            "refer: 0.2497\n",
            "complain: 0.2473\n",
            "roam: 0.2446\n",
            "ask: 0.2422\n",
            "\n",
            "Nearest neighbors for 'argue' (w = 6):\n",
            "consider: 0.2901\n",
            "suggest: 0.2790\n",
            "understand: 0.2745\n",
            "opinions: 0.2699\n",
            "discuss: 0.2663\n",
            "pov: 0.2658\n",
            "believe: 0.2651\n",
            "agree: 0.2642\n",
            "concerns: 0.2612\n",
            "admins: 0.2597\n",
            "\n",
            "Analyzing adjectives words:\n",
            "\n",
            "Query word: 'big'\n",
            "Nearest neighbors for 'big' (w = 1):\n",
            "little: 0.2597\n",
            "large: 0.2583\n",
            "small: 0.2581\n",
            "biggest: 0.2353\n",
            "huge: 0.2336\n",
            "major: 0.2267\n",
            "black: 0.2079\n",
            "mountain: 0.2075\n",
            "rock: 0.2005\n",
            "top: 0.1984\n",
            "\n",
            "Nearest neighbors for 'big' (w = 6):\n",
            "rock: 0.2967\n",
            "featuring: 0.2960\n",
            "star: 0.2884\n",
            "featured: 0.2873\n",
            "hit: 0.2793\n",
            "movie: 0.2754\n",
            "blue: 0.2752\n",
            "tv: 0.2735\n",
            "boy: 0.2700\n",
            "studio: 0.2691\n",
            "\n",
            "Query word: 'important'\n",
            "Nearest neighbors for 'important' (w = 1):\n",
            "significant: 0.3718\n",
            "notable: 0.3669\n",
            "major: 0.3590\n",
            "key: 0.3390\n",
            "interesting: 0.3215\n",
            "main: 0.3055\n",
            "specific: 0.3008\n",
            "valuable: 0.3006\n",
            "various: 0.2932\n",
            "useful: 0.2872\n",
            "\n",
            "Nearest neighbors for 'important' (w = 6):\n",
            "significant: 0.3757\n",
            "especially: 0.3182\n",
            "culture: 0.3078\n",
            "common: 0.3044\n",
            "particularly: 0.3035\n",
            "related: 0.3008\n",
            "notable: 0.2926\n",
            "often: 0.2847\n",
            "example: 0.2841\n",
            "such: 0.2833\n",
            "\n",
            "Analyzing prepositions words:\n",
            "\n",
            "Query word: 'in'\n",
            "Nearest neighbors for 'in' (w = 1):\n",
            "at: 0.3925\n",
            "from: 0.3523\n",
            ".: 0.3415\n",
            "on: 0.2962\n",
            ",: 0.2803\n",
            "): 0.2744\n",
            "until: 0.2717\n",
            "during: 0.2710\n",
            "between: 0.2495\n",
            "since: 0.2480\n",
            "\n",
            "Nearest neighbors for 'in' (w = 6):\n",
            "at: 0.3532\n",
            ",: 0.3530\n",
            "from: 0.3362\n",
            "during: 0.3076\n",
            "until: 0.2860\n",
            "first: 0.2820\n",
            "graduated: 0.2812\n",
            "was: 0.2734\n",
            "championships: 0.2733\n",
            "moved: 0.2705\n",
            "\n",
            "Query word: 'on'\n",
            "Nearest neighbors for 'on' (w = 1):\n",
            "at: 0.3518\n",
            "in: 0.2962\n",
            "as: 0.2837\n",
            "with: 0.2507\n",
            "is: 0.2259\n",
            "which: 0.1906\n",
            "has: 0.1851\n",
            "from: 0.1826\n",
            "but: 0.1612\n",
            "first: 0.1595\n",
            "\n",
            "Nearest neighbors for 'on' (w = 6):\n",
            "september: 0.2721\n",
            "january: 0.2683\n",
            "april: 0.2636\n",
            "march: 0.2633\n",
            "december: 0.2614\n",
            "october: 0.2614\n",
            "july: 0.2578\n",
            "november: 0.2548\n",
            "june: 0.2529\n",
            "august: 0.2486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compare nearest neighbors for multisense word \"bank\"\n",
        "multisense_word = \"bank\"\n",
        "compare_nearest_neighbors(multisense_word, pmi_word_vectors_w1, pmi_word_vectors_w6, vocab_V, vocab_VC)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ixy779h2Fhek",
        "outputId": "77ee7ea4-f56f-4a3e-9574-e9d70b8a4abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest neighbors for 'bank' (w = 1):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-7e71a4f1c0d2>:12: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "railway: 0.2061\n",
            "side: 0.2032\n",
            "coast: 0.2006\n",
            "banks: 0.2002\n",
            "park: 0.1996\n",
            "corporation: 0.1933\n",
            "africa: 0.1906\n",
            "property: 0.1874\n",
            "railroad: 0.1855\n",
            "insurance: 0.1816\n",
            "\n",
            "Nearest neighbors for 'bank' (w = 6):\n",
            "corporation: 0.3778\n",
            "capital: 0.3674\n",
            "railway: 0.3391\n",
            "banks: 0.3227\n",
            "branch: 0.3221\n",
            "valley: 0.3181\n",
            "southern: 0.3159\n",
            "trade: 0.3111\n",
            "largest: 0.3105\n",
            "centre: 0.3098\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}