{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWvSq/IFT9LqksGkv9WDpv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishesh711/AMS_691_NLP/blob/main/AMS691_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Distributional Counting"
      ],
      "metadata": {
        "id": "-T2bsmRdmGn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 Implement Distributional Counting"
      ],
      "metadata": {
        "id": "_hzPkpOBmMTy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUf9GmMyje-d",
        "outputId": "96470e85-1eb4-41e2-89a6-c555cfdd06e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w=3: (chicken, the) = 52\n",
            "w=6: (chicken, the) = 103\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def load_vocab(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        vocab = set([line.strip() for line in file])\n",
        "    return vocab\n",
        "\n",
        "def count_word_pairs(corpus_file, vocab_V, vocab_VC, window_size):\n",
        "    # Sparse data structure to store counts\n",
        "    word_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    # Process the corpus\n",
        "    with open(corpus_file, 'r') as file:\n",
        "        for line in file:\n",
        "            words = line.strip().split()  # Tokenize the sentence\n",
        "            for i, word in enumerate(words):\n",
        "                if word not in vocab_V:\n",
        "                    continue\n",
        "\n",
        "                # Define context window\n",
        "                start = max(i - window_size, 0)\n",
        "                end = min(i + window_size + 1, len(words))\n",
        "\n",
        "                # Count co-occurrences\n",
        "                for j in range(start, end):\n",
        "                    if i == j or words[j] not in vocab_VC:\n",
        "                        continue\n",
        "                    word_counts[word][words[j]] += 1\n",
        "\n",
        "    return word_counts\n",
        "\n",
        "# Load vocabularies\n",
        "vocab_V = load_vocab('/content/vocab-15kws.txt')\n",
        "vocab_VC = load_vocab('/content/vocab-5k.txt')\n",
        "\n",
        "# Perform word pair counting for w = 3 and w = 6\n",
        "word_counts_w3 = count_word_pairs('/content/wiki-1percent.txt', vocab_V, vocab_VC, 3)\n",
        "word_counts_w6 = count_word_pairs('/content/wiki-1percent.txt', vocab_V, vocab_VC, 6)\n",
        "\n",
        "# Example: Output the counts for specific word pairs\n",
        "print(f\"w=3: (chicken, the) = {word_counts_w3['chicken']['the']}\")\n",
        "print(f\"w=6: (chicken, the) = {word_counts_w6['chicken']['the']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2"
      ],
      "metadata": {
        "id": "RmCoDg8oB-F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract and print counts for specific word pairs\n",
        "word_pairs = [\n",
        "    ('chicken', 'the'),\n",
        "    ('chicken', 'wings'),\n",
        "    ('chicago', 'chicago'),\n",
        "    ('coffee', 'the'),\n",
        "    ('coffee', 'cup'),\n",
        "    ('coffee', 'coffee')\n",
        "]\n",
        "\n",
        "print(\"Counts for w = 3\")\n",
        "for word1, word2 in word_pairs:\n",
        "    count_w3 = word_counts_w3.get(word1, {}).get(word2, 0)\n",
        "    print(f\"({word1}, {word2}) = {count_w3}\")\n",
        "\n",
        "print(\"\\nCounts for w = 6\")\n",
        "for word1, word2 in word_pairs:\n",
        "    count_w6 = word_counts_w6.get(word1, {}).get(word2, 0)\n",
        "    print(f\"({word1}, {word2}) = {count_w6}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhpzgiGImUWl",
        "outputId": "cbb108fb-5787-44ab-ed30-b04626d3492e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts for w = 3\n",
            "(chicken, the) = 52\n",
            "(chicken, wings) = 6\n",
            "(chicago, chicago) = 38\n",
            "(coffee, the) = 95\n",
            "(coffee, cup) = 10\n",
            "(coffee, coffee) = 4\n",
            "\n",
            "Counts for w = 6\n",
            "(chicken, the) = 103\n",
            "(chicken, wings) = 7\n",
            "(chicago, chicago) = 122\n",
            "(coffee, the) = 201\n",
            "(coffee, cup) = 14\n",
            "(coffee, coffee) = 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3"
      ],
      "metadata": {
        "id": "CiKoQIBQCRme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Function to compute cosine similarity between two word vectors\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = sum(vec1[key] * vec2.get(key, 0) for key in vec1)\n",
        "    norm1 = np.sqrt(sum(val ** 2 for val in vec1.values()))\n",
        "    norm2 = np.sqrt(sum(val ** 2 for val in vec2.values()))\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0\n",
        "    return dot_product / (norm1 * norm2)\n",
        "\n",
        "# Function to load word similarity datasets (MEN or SimLex-999)\n",
        "def load_similarity_dataset(file_path):\n",
        "    word_pairs = []\n",
        "    human_scores = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        next(file)  # Skip the header\n",
        "        for line in file:\n",
        "            word1, word2, score = line.strip().split()\n",
        "            word_pairs.append((word1, word2))\n",
        "            human_scores.append(float(score))\n",
        "    return word_pairs, human_scores\n",
        "\n",
        "# Function to get word vector from counts\n",
        "def get_word_vector(word, word_counts):\n",
        "    return word_counts.get(word, {})\n",
        "\n",
        "# Function to evaluate word vectors on a dataset\n",
        "def evaluate_word_vectors(dataset_path, word_counts):\n",
        "    word_pairs, human_scores = load_similarity_dataset(dataset_path)\n",
        "    model_scores = []\n",
        "\n",
        "    for word1, word2 in word_pairs:\n",
        "        vec1 = get_word_vector(word1, word_counts)\n",
        "        vec2 = get_word_vector(word2, word_counts)\n",
        "        similarity = cosine_similarity(vec1, vec2)\n",
        "        model_scores.append(similarity)\n",
        "\n",
        "    # Compute Spearman's rank correlation\n",
        "    spearman_corr, _ = spearmanr(human_scores, model_scores)\n",
        "    return spearman_corr\n",
        "\n",
        "# Load MEN and SimLex-999 datasets\n",
        "men_dataset = '/content/men.txt'\n",
        "simlex_dataset = '/content/simlex-999.txt'\n",
        "\n",
        "# Evaluate word vectors (for w = 3, using vocab-15kws.txt for V and vocab-5k.txt for VC)\n",
        "spearman_corr_men = evaluate_word_vectors(men_dataset, word_counts_w3)\n",
        "spearman_corr_simlex = evaluate_word_vectors(simlex_dataset, word_counts_w3)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Spearman correlation for MEN dataset (w = 3): {spearman_corr_men}\")\n",
        "print(f\"Spearman correlation for SimLex-999 dataset (w = 3): {spearman_corr_simlex}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgLbovd9B_UM",
        "outputId": "cede61a0-ad28-4d37-85a9-074cf44dfff8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman correlation for MEN dataset (w = 3): 0.2251396048448754\n",
            "Spearman correlation for SimLex-999 dataset (w = 3): 0.05876135331349779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part 2: Combining Counts with Inverse Document Frequency (IDF)"
      ],
      "metadata": {
        "id": "thVXesDUCb3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1"
      ],
      "metadata": {
        "id": "JMeU_ayKCfUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "# Function to count occurrences of each word in sentences\n",
        "def count_word_sentence_occurrences(corpus_file):\n",
        "    sentence_counts = defaultdict(int)\n",
        "    with open(corpus_file, 'r') as file:\n",
        "        for line in file:\n",
        "            words = set(line.strip().split())  # Treat sentence as a set of unique words\n",
        "            for word in words:\n",
        "                sentence_counts[word] += 1\n",
        "    return sentence_counts\n",
        "\n",
        "# Function to calculate IDF-weighted word vectors\n",
        "def calculate_idf_weighted_vectors(corpus_file, word_counts, vocab_VC):\n",
        "    # Count the total number of sentences\n",
        "    total_sentences = sum(1 for _ in open(corpus_file, 'r'))\n",
        "\n",
        "    # Count the number of sentences each word in the context vocabulary appears in\n",
        "    sentence_counts = count_word_sentence_occurrences(corpus_file)\n",
        "\n",
        "    # Calculate IDF-weighted vectors\n",
        "    idf_weighted_vectors = defaultdict(lambda: defaultdict(float))\n",
        "    for word_x, context_dict in word_counts.items():\n",
        "        for word_y, count in context_dict.items():\n",
        "            # Compute IDF\n",
        "            idf = total_sentences / (sentence_counts[word_y] + 1)  # Avoid division by zero\n",
        "            # Apply IDF to the count\n",
        "            idf_weighted_vectors[word_x][word_y] = count * idf\n",
        "\n",
        "    return idf_weighted_vectors\n",
        "\n",
        "# Calculate the IDF-weighted vectors for w = 3\n",
        "idf_word_counts_w3 = calculate_idf_weighted_vectors('/content/wiki-1percent.txt', word_counts_w3, vocab_VC)\n",
        "idf_word_counts_w6 = calculate_idf_weighted_vectors('/content/wiki-1percent.txt', word_counts_w6, vocab_VC)\n",
        "\n",
        "# Evaluate the IDF-weighted word vectors using EVALWS (Spearman correlation)\n",
        "spearman_corr_idf_men_w3 = evaluate_word_vectors(men_dataset, idf_word_counts_w3)\n",
        "spearman_corr_idf_simlex_w3 = evaluate_word_vectors(simlex_dataset, idf_word_counts_w3)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Spearman correlation for MEN dataset (w = 3, IDF): {spearman_corr_idf_men_w3}\")\n",
        "print(f\"Spearman correlation for SimLex-999 dataset (w = 3, IDF): {spearman_corr_idf_simlex_w3}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1Xp0xftCUQa",
        "outputId": "a10e18e5-2621-4af1-9a99-6451e0c994a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman correlation for MEN dataset (w = 3, IDF): 0.47285107067097226\n",
            "Spearman correlation for SimLex-999 dataset (w = 3, IDF): 0.16436460954112178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part 3: Pointwise Mutual Information (PMI)"
      ],
      "metadata": {
        "id": "j4sGT6F7C2AC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1"
      ],
      "metadata": {
        "id": "QZzT3AmhC51M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from math import log2\n",
        "\n",
        "# Function to calculate PMI values for word pairs\n",
        "def calculate_pmi(word_counts, vocab_V, vocab_VC):\n",
        "    total_count = sum(sum(context_dict.values()) for context_dict in word_counts.values())  # Total N\n",
        "    pmi_vectors = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    # Compute marginal probabilities\n",
        "    p_x = defaultdict(float)\n",
        "    p_y = defaultdict(float)\n",
        "\n",
        "    for word_x, context_dict in word_counts.items():\n",
        "        p_x[word_x] = sum(context_dict.values()) / total_count\n",
        "        for word_y, count in context_dict.items():\n",
        "            p_y[word_y] += count / total_count\n",
        "\n",
        "    # Compute PMI for each word pair\n",
        "    for word_x, context_dict in word_counts.items():\n",
        "        for word_y, count in context_dict.items():\n",
        "            joint_prob = count / total_count\n",
        "            if joint_prob > 0 and p_x[word_x] > 0 and p_y[word_y] > 0:\n",
        "                pmi = log2(joint_prob / (p_x[word_x] * p_y[word_y]))\n",
        "                pmi_vectors[word_x][word_y] = pmi\n",
        "\n",
        "    return pmi_vectors\n",
        "\n",
        "# Calculate PMI-based word vectors for w = 3\n",
        "pmi_word_vectors_w3 = calculate_pmi(word_counts_w3, vocab_V, vocab_VC)\n",
        "\n",
        "# Calculate PMI-based word vectors for w = 6\n",
        "pmi_word_vectors_w6 = calculate_pmi(word_counts_w6, vocab_V, vocab_VC)\n",
        "\n",
        "# Function to print the top 10 context words with the largest and smallest PMI values for \"coffee\"\n",
        "def print_top_pmi_words(word, pmi_vectors, top_n=10):\n",
        "    if word not in pmi_vectors:\n",
        "        print(f\"Word '{word}' not found in PMI vectors.\")\n",
        "        return\n",
        "\n",
        "    sorted_pmi = sorted(pmi_vectors[word].items(), key=lambda x: x[1], reverse=True)\n",
        "    print(f\"\\nTop {top_n} words with highest PMI for '{word}':\")\n",
        "    for context_word, pmi_value in sorted_pmi[:top_n]:\n",
        "        print(f\"{context_word}: {pmi_value:.4f}\")\n",
        "\n",
        "    print(f\"\\nTop {top_n} words with lowest PMI for '{word}':\")\n",
        "    for context_word, pmi_value in sorted_pmi[-top_n:]:\n",
        "        print(f\"{context_word}: {pmi_value:.4f}\")\n",
        "\n",
        "# Print top 10 words with highest and lowest PMI for \"coffee\"\n",
        "print_top_pmi_words('coffee', pmi_word_vectors_w3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEXcyYhkCl6i",
        "outputId": "7697733c-e48f-4804-9453-47e50c724acd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 words with highest PMI for 'coffee':\n",
            "tea: 8.1660\n",
            "drinking: 7.5880\n",
            "shop: 7.4117\n",
            "costa: 7.3503\n",
            "shops: 7.2608\n",
            "sugar: 6.5339\n",
            "coffee: 6.5020\n",
            "mix: 6.1312\n",
            "seattle: 5.9508\n",
            "houses: 5.8682\n",
            "\n",
            "Top 10 words with lowest PMI for 'coffee':\n",
            "page: -1.2806\n",
            "when: -1.4043\n",
            "more: -1.4785\n",
            "after: -1.5985\n",
            "its: -1.8395\n",
            "not: -1.9116\n",
            "this: -1.9795\n",
            "had: -1.9875\n",
            "be: -2.1510\n",
            "he: -2.2603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2"
      ],
      "metadata": {
        "id": "GD5OkY5wC-52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate PMI-based word vectors using EVALWS (Spearman correlation)\n",
        "spearman_corr_pmi_men_w3 = evaluate_word_vectors(men_dataset, pmi_word_vectors_w3)\n",
        "spearman_corr_pmi_simlex_w3 = evaluate_word_vectors(simlex_dataset, pmi_word_vectors_w3)\n",
        "\n",
        "# Print the results for PMI-based word vectors\n",
        "print(f\"Spearman correlation for MEN dataset (w = 3, PMI): {spearman_corr_pmi_men_w3}\")\n",
        "print(f\"Spearman correlation for SimLex-999 dataset (w = 3, PMI): {spearman_corr_pmi_simlex_w3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWWmSjvRC8wZ",
        "outputId": "112ace45-e58b-4f70-bb42-3e5b747b7d59"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman correlation for MEN dataset (w = 3, PMI): 0.46563240836038006\n",
            "Spearman correlation for SimLex-999 dataset (w = 3, PMI): 0.18643183126956037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part 4: Quantitative Comparisons"
      ],
      "metadata": {
        "id": "PcikvuB8DL9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1 and 4.2"
      ],
      "metadata": {
        "id": "XZet66v6De9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate word vectors across different methods and window sizes\n",
        "def evaluate_all_methods(men_dataset, simlex_dataset, word_counts_w1, word_counts_w3, word_counts_w6,\n",
        "                         idf_word_counts_w1, idf_word_counts_w3, idf_word_counts_w6,\n",
        "                         pmi_word_vectors_w1, pmi_word_vectors_w3, pmi_word_vectors_w6):\n",
        "\n",
        "    # List to store results\n",
        "    results = []\n",
        "\n",
        "    # Loop through each method and window size\n",
        "    for method, word_vectors_w1, word_vectors_w3, word_vectors_w6 in [\n",
        "        ('Counts', word_counts_w1, word_counts_w3, word_counts_w6),\n",
        "        ('IDF', idf_word_counts_w1, idf_word_counts_w3, idf_word_counts_w6),\n",
        "        ('PMI', pmi_word_vectors_w1, pmi_word_vectors_w3, pmi_word_vectors_w6)]:\n",
        "\n",
        "        for w, word_vectors in [(1, word_vectors_w1), (3, word_vectors_w3), (6, word_vectors_w6)]:\n",
        "            # Evaluate on MEN dataset\n",
        "            spearman_corr_men = evaluate_word_vectors(men_dataset, word_vectors)\n",
        "            # Evaluate on SimLex-999 dataset\n",
        "            spearman_corr_simlex = evaluate_word_vectors(simlex_dataset, word_vectors)\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                'Method': method,\n",
        "                'Window Size': w,\n",
        "                'MEN Correlation': spearman_corr_men,\n",
        "                'SimLex Correlation': spearman_corr_simlex\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Now run the evaluation for all combinations of methods and window sizes\n",
        "results = evaluate_all_methods(\n",
        "    men_dataset, simlex_dataset,\n",
        "    word_counts_w1, word_counts_w3, word_counts_w6,  # Raw counts for w = 1, 3, 6\n",
        "    idf_word_counts_w1, idf_word_counts_w3, idf_word_counts_w6,  # IDF for w = 1, 3, 6\n",
        "    pmi_word_vectors_w1, pmi_word_vectors_w3, pmi_word_vectors_w6  # PMI for w = 1, 3, 6\n",
        ")\n",
        "\n",
        "# Display results in a structured format\n",
        "import pandas as pd\n",
        "df_results = pd.DataFrame(results)\n",
        "print(df_results)\n",
        "\n",
        "# Analyze the results for trends\n",
        "\n",
        "def analyze_trends(df_results):\n",
        "    # Find the best performing method for each window size\n",
        "    best_method_men = df_results.groupby('Window Size')['MEN Correlation'].idxmax()\n",
        "    best_method_simlex = df_results.groupby('Window Size')['SimLex Correlation'].idxmax()\n",
        "\n",
        "    # Display best methods for each window size for MEN and SimLex\n",
        "    print(\"\\nBest methods for MEN dataset by window size:\")\n",
        "    print(df_results.loc[best_method_men])\n",
        "\n",
        "    print(\"\\nBest methods for SimLex-999 dataset by window size:\")\n",
        "    print(df_results.loc[best_method_simlex])\n",
        "\n",
        "    # Average performance of each method\n",
        "    avg_performance = df_results.groupby('Method').mean()\n",
        "    print(\"\\nAverage performance of each method across all window sizes:\")\n",
        "    print(avg_performance)\n",
        "\n",
        "# Analyze the trends\n",
        "analyze_trends(df_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0Dzg_6lDN1Z",
        "outputId": "139caee5-e879-4080-e08f-79555be54ba8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Method  Window Size  MEN Correlation  SimLex Correlation\n",
            "0  Counts            1         0.209092            0.067786\n",
            "1  Counts            3         0.225140            0.058761\n",
            "2  Counts            6         0.241067            0.044696\n",
            "3     IDF            1         0.347589            0.189255\n",
            "4     IDF            3         0.472851            0.164365\n",
            "5     IDF            6         0.532401            0.110635\n",
            "6     PMI            1         0.433603            0.227498\n",
            "7     PMI            3         0.465632            0.186432\n",
            "8     PMI            6         0.472408            0.150331\n",
            "\n",
            "Best methods for MEN dataset by window size:\n",
            "  Method  Window Size  MEN Correlation  SimLex Correlation\n",
            "6    PMI            1         0.433603            0.227498\n",
            "4    IDF            3         0.472851            0.164365\n",
            "5    IDF            6         0.532401            0.110635\n",
            "\n",
            "Best methods for SimLex-999 dataset by window size:\n",
            "  Method  Window Size  MEN Correlation  SimLex Correlation\n",
            "6    PMI            1         0.433603            0.227498\n",
            "7    PMI            3         0.465632            0.186432\n",
            "8    PMI            6         0.472408            0.150331\n",
            "\n",
            "Average performance of each method across all window sizes:\n",
            "        Window Size  MEN Correlation  SimLex Correlation\n",
            "Method                                                  \n",
            "Counts     3.333333         0.225099            0.057081\n",
            "IDF        3.333333         0.450947            0.154751\n",
            "PMI        3.333333         0.457215            0.188087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5 Qualitative Analysis (25 points)"
      ],
      "metadata": {
        "id": "GwHOu9tiG82P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.1"
      ],
      "metadata": {
        "id": "Yn5X1IGeHAm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the k nearest neighbors of a word\n",
        "def get_nearest_neighbors(word, word_vectors, vocab, k=10):\n",
        "    if word not in word_vectors:\n",
        "        print(f\"Word '{word}' not found in word vectors.\")\n",
        "        return []\n",
        "\n",
        "    similarities = []\n",
        "    for other_word in vocab:\n",
        "        if other_word != word and other_word in word_vectors:\n",
        "            similarity = cosine_similarity(word_vectors[word], word_vectors[other_word])\n",
        "            similarities.append((other_word, similarity))\n",
        "\n",
        "    # Sort by similarity and return the top k\n",
        "    nearest_neighbors = sorted(similarities, key=lambda x: x[1], reverse=True)[:k]\n",
        "    return nearest_neighbors\n",
        "\n",
        "# Function to print the nearest neighbors for a word with two different window sizes\n",
        "def compare_nearest_neighbors(word, word_vectors_w1, word_vectors_w6, vocab, k=10):\n",
        "    print(f\"Nearest neighbors for '{word}' (w = 1):\")\n",
        "    neighbors_w1 = get_nearest_neighbors(word, word_vectors_w1, vocab, k)\n",
        "    for neighbor, similarity in neighbors_w1:\n",
        "        print(f\"{neighbor}: {similarity:.4f}\")\n",
        "\n",
        "    print(f\"\\nNearest neighbors for '{word}' (w = 6):\")\n",
        "    neighbors_w6 = get_nearest_neighbors(word, word_vectors_w6, vocab, k)\n",
        "    for neighbor, similarity in neighbors_w6:\n",
        "        print(f\"{neighbor}: {similarity:.4f}\")\n",
        "\n",
        "# Example: Compare nearest neighbors for the word \"judges\"\n",
        "word = \"judges\"\n",
        "compare_nearest_neighbors(word, pmi_word_vectors_w1, pmi_word_vectors_w6, vocab_V)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzZlR26gG7zt",
        "outputId": "85300cb0-f9ed-4552-adbd-1822405b4b35"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest neighbors for 'judges' (w = 1):\n",
            "judge: 0.2225\n",
            "players: 0.2115\n",
            "appeals: 0.1893\n",
            "officials: 0.1817\n",
            "ministers: 0.1786\n",
            "justices: 0.1784\n",
            "leaders: 0.1729\n",
            "members: 0.1729\n",
            "unanimously: 0.1700\n",
            "contestants: 0.1657\n",
            "\n",
            "Nearest neighbors for 'judges' (w = 6):\n",
            "judge: 0.3054\n",
            "jury: 0.2880\n",
            "appeals: 0.2774\n",
            "courts: 0.2740\n",
            "panel: 0.2740\n",
            "supreme: 0.2688\n",
            "justice: 0.2566\n",
            "contestants: 0.2565\n",
            "candidates: 0.2488\n",
            "appeal: 0.2488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.2"
      ],
      "metadata": {
        "id": "1EEiu1Q7HXZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compare nearest neighbors for a list of words with two different window sizes\n",
        "def compare_neighbors_by_pos(words, word_vectors_w1, word_vectors_w6, vocab, k=10):\n",
        "    for word in words:\n",
        "        print(f\"\\nComparing nearest neighbors for '{word}':\")\n",
        "        compare_nearest_neighbors(word, word_vectors_w1, word_vectors_w6, vocab, k)\n",
        "\n",
        "# List of words from different parts of speech\n",
        "query_words = {\n",
        "    'Noun': 'judge',\n",
        "    'Verb': 'run',\n",
        "    'Adjective': 'happy',\n",
        "    'Preposition': 'above'\n",
        "}\n",
        "\n",
        "# Example: Compare nearest neighbors for words from different parts of speech\n",
        "for pos, word in query_words.items():\n",
        "    print(f\"\\n=== Part of Speech: {pos} ===\")\n",
        "    compare_neighbors_by_pos([word], pmi_word_vectors_w1, pmi_word_vectors_w6, vocab_V)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hheQmRZHW9m",
        "outputId": "0052a21f-f84d-4883-af09-dd908d6d2e2d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Part of Speech: Noun ===\n",
            "\n",
            "Comparing nearest neighbors for 'judge':\n",
            "Nearest neighbors for 'judge' (w = 1):\n",
            "captain: 0.2652\n",
            "justice: 0.2505\n",
            "lieutenant: 0.2360\n",
            "smith: 0.2338\n",
            "professor: 0.2287\n",
            "sir: 0.2276\n",
            "court: 0.2266\n",
            "king: 0.2238\n",
            "henry: 0.2236\n",
            "george: 0.2227\n",
            "\n",
            "Nearest neighbors for 'judge' (w = 6):\n",
            "justice: 0.4014\n",
            "supreme: 0.4009\n",
            "attorney: 0.3780\n",
            "court: 0.3560\n",
            "governor: 0.3530\n",
            "appeals: 0.3246\n",
            "mayor: 0.3242\n",
            "lawyer: 0.3181\n",
            "deputy: 0.3166\n",
            "secretary: 0.3158\n",
            "\n",
            "=== Part of Speech: Verb ===\n",
            "\n",
            "Comparing nearest neighbors for 'run':\n",
            "Nearest neighbors for 'run' (w = 1):\n",
            "runs: 0.2770\n",
            "running: 0.2723\n",
            "operate: 0.2373\n",
            "pass: 0.2269\n",
            "go: 0.2195\n",
            "operated: 0.2184\n",
            "ran: 0.2081\n",
            "come: 0.2012\n",
            "held: 0.1946\n",
            "move: 0.1926\n",
            "\n",
            "Nearest neighbors for 'run' (w = 6):\n",
            "running: 0.3625\n",
            "runs: 0.3173\n",
            "ran: 0.2953\n",
            "start: 0.2534\n",
            "pass: 0.2519\n",
            "race: 0.2436\n",
            "away: 0.2394\n",
            "drive: 0.2336\n",
            "car: 0.2335\n",
            "play: 0.2273\n",
            "\n",
            "=== Part of Speech: Adjective ===\n",
            "\n",
            "Comparing nearest neighbors for 'happy':\n",
            "Nearest neighbors for 'happy' (w = 1):\n",
            "pleased: 0.2962\n",
            "surprised: 0.2588\n",
            "worried: 0.2491\n",
            "glad: 0.2435\n",
            "sorry: 0.2428\n",
            "afraid: 0.2381\n",
            "proud: 0.2369\n",
            "satisfied: 0.2232\n",
            "willing: 0.2105\n",
            "sure: 0.2037\n",
            "\n",
            "Nearest neighbors for 'happy' (w = 6):\n",
            "anyone: 0.3360\n",
            "'ll: 0.3323\n",
            "everyone: 0.3265\n",
            "'d: 0.3230\n",
            "let: 0.3150\n",
            "ask: 0.3136\n",
            "feel: 0.3131\n",
            "wants: 0.3073\n",
            "hope: 0.3035\n",
            "saying: 0.3033\n",
            "\n",
            "=== Part of Speech: Preposition ===\n",
            "\n",
            "Comparing nearest neighbors for 'above':\n",
            "Nearest neighbors for 'above' (w = 1):\n",
            "below: 0.4373\n",
            "here: 0.2940\n",
            "debate: 0.2718\n",
            "following: 0.2660\n",
            "list: 0.2632\n",
            "review: 0.2609\n",
            "however: 0.2594\n",
            "talk: 0.2551\n",
            "link: 0.2497\n",
            "same: 0.2461\n",
            "\n",
            "Nearest neighbors for 'above' (w = 6):\n",
            "page: 0.4822\n",
            "should: 0.4616\n",
            "discussion: 0.4587\n",
            "talk: 0.4548\n",
            "below: 0.4352\n",
            "link: 0.4228\n",
            "article: 0.4218\n",
            "here: 0.4211\n",
            "do: 0.4157\n",
            "review: 0.4041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.3**"
      ],
      "metadata": {
        "id": "d7wfSPGsFrsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to compute cosine similarity between two word vectors\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = sum(vec1[key] * vec2.get(key, 0) for key in vec1)\n",
        "    norm1 = np.sqrt(sum(val ** 2 for val in vec1.values()))\n",
        "    norm2 = np.sqrt(sum(val ** 2 for val in vec2.values()))\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0\n",
        "    return dot_product / (norm1 * norm2)\n",
        "\n",
        "# Function to get the k nearest neighbors of a word\n",
        "def get_nearest_neighbors(word, word_vectors, vocab, k=10):\n",
        "    if word not in word_vectors:\n",
        "        print(f\"Word '{word}' not found in word vectors.\")\n",
        "        return []\n",
        "\n",
        "    similarities = []\n",
        "    for other_word in vocab:\n",
        "        if other_word != word and other_word in word_vectors:\n",
        "            similarity = cosine_similarity(word_vectors[word], word_vectors[other_word])\n",
        "            similarities.append((other_word, similarity))\n",
        "\n",
        "    # Sort by similarity and return the top k\n",
        "    nearest_neighbors = sorted(similarities, key=lambda x: x[1], reverse=True)[:k]\n",
        "    return nearest_neighbors\n",
        "\n",
        "# Function to print the nearest neighbors for a word with two different window sizes\n",
        "def compare_nearest_neighbors(word, word_vectors_w1, word_vectors_w6, vocab, k=10):\n",
        "    print(f\"Nearest neighbors for '{word}' (w = 1):\")\n",
        "    neighbors_w1 = get_nearest_neighbors(word, word_vectors_w1, vocab, k)\n",
        "    for neighbor, similarity in neighbors_w1:\n",
        "        print(f\"{neighbor}: {similarity:.4f}\")\n",
        "\n",
        "    print(f\"\\nNearest neighbors for '{word}' (w = 6):\")\n",
        "    neighbors_w6 = get_nearest_neighbors(word, word_vectors_w6, vocab, k)\n",
        "    for neighbor, similarity in neighbors_w6:\n",
        "        print(f\"{neighbor}: {similarity:.4f}\")\n",
        "\n",
        "# Example: Compare nearest neighbors for multisense word \"bank\"\n",
        "multisense_word = \"bank\"\n",
        "compare_nearest_neighbors(multisense_word, pmi_word_vectors_w1, pmi_word_vectors_w6, vocab_V)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIpdO0GdDqWj",
        "outputId": "d75a6450-dedb-4e41-a17f-6d9aaa4ec444"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest neighbors for 'bank' (w = 1):\n",
            "side: 0.2065\n",
            "coast: 0.2063\n",
            "railway: 0.2063\n",
            "park: 0.2022\n",
            "africa: 0.1993\n",
            "banks: 0.1965\n",
            "corporation: 0.1927\n",
            "property: 0.1870\n",
            "railroad: 0.1847\n",
            "province: 0.1836\n",
            "\n",
            "Nearest neighbors for 'bank' (w = 6):\n",
            "capital: 0.3709\n",
            "corporation: 0.3707\n",
            "railway: 0.3417\n",
            "northern: 0.3262\n",
            "branch: 0.3194\n",
            "southern: 0.3179\n",
            "valley: 0.3177\n",
            "lake: 0.3144\n",
            "banks: 0.3144\n",
            "centre: 0.3120\n"
          ]
        }
      ]
    }
  ]
}